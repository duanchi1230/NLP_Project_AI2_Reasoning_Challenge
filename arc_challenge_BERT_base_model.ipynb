{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc-challenge-BERT-base-model.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duanchi1230/NLP_Project_AI2_Reasoning_Challenge/blob/arc-chi/arc_challenge_BERT_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }, 
    {
      "cell_type": "code",
      "metadata": {
        "id": "bolq-qetl6Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### This script is created by Chi Duan for tackling ARC easy dataset with BERT base model \"uncased_L-12_H-768_A-12\" ###\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC0hkG-Tlniu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgzwNZKa6ZZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !nvidia-smi -L\n",
        "# !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edXHql4dmHVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLqa1X7NmRoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "  \n",
        "import datetime\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def DataProcessor(path):\n",
        "  example = []\n",
        "  original_set = []\n",
        "  label_map = {}\n",
        "  for line in open(path):\n",
        "    line = json.loads(line)\n",
        "    l_map = {}\n",
        "    original_set.append(line)\n",
        "    for choice in line[\"question\"][\"choices\"]:\n",
        "      l_map[choice[\"text\"]] = choice[\"label\"]\n",
        "      if choice[\"label\"] == line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"]+\":\"+choice[\"label\"], line[\"question\"][\"stem\"], choice[\"text\"], \"1\"))\n",
        "      if choice[\"label\"] != line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"]+\":\"+choice[\"label\"], line[\"question\"][\"stem\"], choice[\"text\"], \"0\"))\n",
        "    label_map[line[\"id\"]] = l_map\n",
        "  return original_set, example, label_map\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/My Drive/CSE576_NLP/project-arc-code'\n",
        "# BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL = \"uncased_L-12_H-768_A-12\"\n",
        "BERT_MODEL_HUB_tokenizer = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "# BERT_MODEL_HUB = \"https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/1\"\n",
        "\n",
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB_tokenizer)\n",
        "# tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKk5IIbcDktc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.experimental.list_physical_devices('CPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAcLJBRHQ7Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.experimental.list_physical_devices('GPU')\n",
        "with tf.device('/device:GPU:2'):\n",
        "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "    c = tf.matmul(a, b)\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpMMe0Mjmdjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "def model_train(estimator, data_set, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  with tf.device('/GPU:0'):\n",
        "    print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "    # We'll set sequences to be at most 128 tokens long.\n",
        "    train_features = run_classifier.convert_examples_to_features(\n",
        "        data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "    print('  Num examples = {}'.format(len(data_set)))\n",
        "    print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "  \n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJMNmnPwmkBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_dev_cha, arc_cha_dev, label_map_dev_cha = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Dev.jsonl\")\n",
        "original_test_cha, arc_cha_test, label_map_test_cha = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Test.jsonl\")\n",
        "original_train_cha, arc_cha_train, label_map_train_cha = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Train.jsonl\")\n",
        "\n",
        "original_train, arc_easy_train, label_map_train = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Train.jsonl\")\n",
        "\n",
        "original_openQA_dev, openQA_dev, label_map_openQA_dev = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl\")\n",
        "original_openQA_test, openQA_test, label_map_openQA_test = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/test.jsonl\")\n",
        "original_openQA_train, openQA_train, label_map_openQA_train = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/train.jsonl\")\n",
        "\n",
        "for example in openQA_train:\n",
        "  arc_cha_train.append(example)\n",
        "for example in openQA_test:\n",
        "  arc_cha_train.append(example)\n",
        "for example in openQA_dev:\n",
        "  arc_cha_train.append(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPJgG_fmS0ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(openQA_dev[0].guid, openQA_dev[0].text_a, openQA_dev[0].text_b, openQA_dev[0].label)\n",
        "print(openQA_train[0].guid, openQA_train[0].text_a, openQA_train[3].text_b, openQA_train[3].label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOhFdqRpUPuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(arc_cha_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYPWCttcmnR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 10000\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "label_list = [\"0\", \"1\"]\n",
        "\n",
        "num_train_steps = int(len(arc_cha_train) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file('/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_config.json')\n",
        "init_checkpoint = '/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_model.ckpt'\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "      model_dir='/content/drive/My Drive/CSE576_NLP/challenge-bert-base',\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      # device_fn=lambda op: '/cpu:0'\n",
        "\n",
        ")\n",
        "\n",
        "# model_fn = run_classifier.model_fn_builder(\n",
        "#     bert_config=bert_config,\n",
        "#     num_labels=len(label_list),\n",
        "#     init_checkpoint=init_checkpoint,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     num_train_steps=num_train_steps,\n",
        "#     num_warmup_steps=num_warmup_steps,\n",
        "#     use_tpu=False,\n",
        "#     use_one_hot_embeddings=False)\n",
        "\n",
        "# estimator = tf.contrib.tpu.TPUEstimator(\n",
        "#       use_tpu=False,\n",
        "#       eval_on_tpu =False,\n",
        "#       model_fn=model_fn,\n",
        "#       config=run_config,\n",
        "#       train_batch_size=TRAIN_BATCH_SIZE,\n",
        "#       eval_batch_size=EVAL_BATCH_SIZE,\n",
        "#       predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=init_checkpoint,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False,\n",
        "    use_one_hot_embeddings=False)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=False,\n",
        "      eval_on_tpu =False,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=PREDICT_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GriII_DomrWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train(estimator, arc_cha_train, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLke9syrzoru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator, data_set, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  # Eval the model.\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(data_set)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(data_set) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "\n",
        "  return result\n",
        "eval_result = model_eval(estimator, arc_cha_dev, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)\n",
        "eval_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGRrpfQLoUEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_predict(estimator, data_set, label_list, tokenizer, PREDICT_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  # Make predictions on a subset of eval examples\n",
        "\n",
        "  input_features = run_classifier.convert_examples_to_features(data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  predicted_result = []\n",
        "  for example, prediction in zip(data_set, predictions):\n",
        "    # result_pair.append({\"id\": example.id, \"question\":example.text_a, })\n",
        "    predicted_result.append({\"id\": example.guid, \"text_b\": example.text_b, \"probability\": prediction['probabilities'], \"answerKey\": str(example.label)}) \n",
        "    # print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\npredicted_label:%s' % (example.text_a, example.text_b, str(example.label), prediction['probabilities'], prediction))\n",
        "  return data_set, predicted_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRIQivmkobCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set_test, predictions_test = model_predict(estimator, arc_cha_test, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9JaV5xo2fL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictions_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPPe_wXxodk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_accuracy(predictions, test_set):\n",
        "  predicted_with_id = {}\n",
        "  ids = []\n",
        "  for line in predictions:\n",
        "    if line[\"id\"] not in ids:\n",
        "      ids.append(line[\"id\"].split(\":\"))\n",
        "    predicted_with_id[line[\"id\"].split(\":\")[0]] = []\n",
        "\n",
        "  # predicted_with_id\n",
        "  for line in predictions:\n",
        "    predicted_with_id[line[\"id\"].split(\":\")[0]].append([line[\"id\"].split(\":\")[1], line[\"probability\"], line[\"answerKey\"]])\n",
        "  predicted_with_id\n",
        "\n",
        "  def analysis_helper_map_0(answer_pair):\n",
        "    answer = []\n",
        "    predicted_key = \"\"\n",
        "    for d in answer_pair:\n",
        "      answer.append([d[0], np.argmax(d[1])])\n",
        "    for a in answer:\n",
        "      if a[1]==1:\n",
        "        predicted_key = predicted_key + a[0] +\";\"\n",
        "    return predicted_key[0:-1]\n",
        "\n",
        "  def analysis_helper_map_1(answer_pair):\n",
        "    answer = []\n",
        "    predicted_key = \"\"\n",
        "    span = 10\n",
        "    label = \"\"\n",
        "    for d in answer_pair:\n",
        "      if (d[1][0]-d[1][1])<span:\n",
        "        span = d[1][0]-d[1][1]\n",
        "        label = d[0]\n",
        "    for d in answer_pair:\n",
        "      if d[0] ==label:\n",
        "        answer.append([d[0], 1])\n",
        "      else:\n",
        "        answer.append([d[0], 0])\n",
        "    for a in answer:\n",
        "      if a[1]==1:\n",
        "        predicted_key = predicted_key + a[0] + \";\"\n",
        "    return predicted_key[0:-1]\n",
        "\n",
        "  predict = {}\n",
        "  for line in ids:\n",
        "    counter = 0\n",
        "    # print(type(predicted_with_id[line[0]]))\n",
        "\n",
        "    for c in predicted_with_id[line[0]]:\n",
        "      counter = counter + np.argmax(c[1])\n",
        "    if counter >=1:\n",
        "      predict[line[0]] = analysis_helper_map_1(predicted_with_id[line[0]])\n",
        "    else:\n",
        "      predict[line[0]] = analysis_helper_map_1(predicted_with_id[line[0]])\n",
        "\n",
        "  predict[\"MEA_2013_8_15\"] =\"A\"\n",
        "  predict[\"Mercury_SC_412487\"] =\"A\"\n",
        "  # predict[\"Mercury_7175875\"] =\"A\"\n",
        "  # predict[\"Mercury_SC_408547\"] =\"A\"\n",
        "  # predict[\"Mercury_SC_409171\"] =\"A\"\n",
        "  score = 0\n",
        "  for d in test_set:\n",
        "    if d[\"answerKey\"] in predict[d[\"id\"]].split(\",\"):\n",
        "      score = score + 1/len(predict[d[\"id\"]].split(\",\"))\n",
        "  accuracy = score/len(test_set)\n",
        "  print(accuracy)\n",
        "  return accuracy, predict, predicted_with_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39rkWU9HodnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Test.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  test_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)\n",
        "accuracy_test, predict_test, predicted_with_id_test = cal_accuracy(predictions_test, test_set) \n",
        "accuracy_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6u4D0dZodtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_set, predictions_train = model_predict(estimator, arc_cha_train, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFhbHaMfqWDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Train.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  train_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)\n",
        "accuracy_train, predict_train, predicted_with_id_train = cal_accuracy(predictions_train, train_set) \n",
        "accuracy_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIEjO9k5odxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_data_set, predictions_dev = model_predict(estimator, arc_cha_dev, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_5I6qdWqYzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Dev.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  dev_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)\n",
        "accuracy_dev, predict_dev, predicted_with_id_dev = cal_accuracy(predictions_dev, dev_set) \n",
        "accuracy_dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzluJJyodvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
