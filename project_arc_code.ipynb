{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_arc_code.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcYueVojnCixdgJMWldwAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duanchi1230/NLP_Project_AI2_Reasoning_Challenge/blob/arc-chi/project_arc_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6i2cjbaQbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-DMTOqWnG01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW4vi0lxmxH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMXfby33mzIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmyhQImrE-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def DataProcessor(path):\n",
        "  example = []\n",
        "  for line in open(path):\n",
        "    line = json.loads(line)\n",
        "    for choice in line[\"question\"][\"choices\"]:\n",
        "      if choice[\"label\"] == line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"], line[\"question\"][\"stem\"], choice[\"text\"], \"1\"))\n",
        "      if choice[\"label\"] != line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"], line[\"question\"][\"stem\"], choice[\"text\"], \"0\"))\n",
        "  return example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEnyGpe5a6ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arc_corpus = open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC_Corpus.txt\",\"r+\")\n",
        "\n",
        "arc_easy_dev = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Dev.jsonl\")\n",
        "arc_easy_test = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Test.jsonl\")\n",
        "arc_easy_train = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Train.jsonl\")\n",
        "\n",
        "# for train_examples in arc_easy_dev:\n",
        "#   print(train_examples.guid) \n",
        "#   print(train_examples.text_a) \n",
        "#   print(train_examples.text_b) \n",
        "#   print(train_examples.label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4nZDhen18Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(arc_easy_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU-K6P2bgAO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = '/content/drive/My Drive/CSE576_NLP/project-arc-code'\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MND33E2Wu6uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "label_list = [\"0\", \"1\"]\n",
        "\n",
        "num_train_steps = int(len(arc_easy_train) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-mfy9zvfpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = modeling.BertConfig.from_json_file('/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_config.json')\n",
        "init_checkpoint = '/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_model.ckpt'\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "      model_dir='/content/drive/My Drive/CSE576_NLP',\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        ")\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=init_checkpoint,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False,\n",
        "    use_one_hot_embeddings=False)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=False,\n",
        "      eval_on_tpu =False,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=PREDICT_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LucH7CCbwWNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "def model_train(estimator, data_set, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(data_set)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqMr_SsewY9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train(estimator, arc_easy_train, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_F2TUdk0E6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator, data_set, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  # Eval the model.\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(data_set)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(data_set) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "\n",
        "  return result\n",
        "  # output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  # with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "  #   print(\"***** Eval results *****\")\n",
        "  #   for key in sorted(result.keys()):\n",
        "  #     print('  {} = {}'.format(key, str(result[key])))\n",
        "  #     writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulcyQsWU0tmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_result = model_eval(estimator, arc_easy_dev, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaP7pWyA1pUG",
        "colab_type": "code",
        "colab": {}
      },
      
      "source": [
        "eval_result"
      ],
      
      "execution_count": 0,
      
      "outputs": ["NA"
                 ,"NA",
                 "NA"]
    }
  ]
}
