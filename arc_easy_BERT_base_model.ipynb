{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arc-easy-BERT-base-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duanchi1230/NLP_Project_AI2_Reasoning_Challenge/blob/arc-chi/arc_easy_BERT_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6i2cjbaQbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shRlD1WuLTFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf3ZCsBTo7Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !nvidia-smi -L\n",
        "# !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-DMTOqWnG01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW4vi0lxmxH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMXfby33mzIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmyhQImrE-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def DataProcessor(path):\n",
        "  example = []\n",
        "  original_set = []\n",
        "  label_map = {}\n",
        "  for line in open(path):\n",
        "    line = json.loads(line)\n",
        "    l_map = {}\n",
        "    original_set.append(line)\n",
        "    for choice in line[\"question\"][\"choices\"]:\n",
        "      l_map[choice[\"text\"]] = choice[\"label\"]\n",
        "      if choice[\"label\"] == line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"]+\":\"+choice[\"label\"], line[\"question\"][\"stem\"], choice[\"text\"], \"1\"))\n",
        "      if choice[\"label\"] != line[\"answerKey\"]:\n",
        "        example.append(run_classifier.InputExample(line[\"id\"]+\":\"+choice[\"label\"], line[\"question\"][\"stem\"], choice[\"text\"], \"0\"))\n",
        "    label_map[line[\"id\"]] = l_map\n",
        "  return original_set, example, label_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEnyGpe5a6ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arc_corpus = open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC_Corpus.txt\",\"r+\")\n",
        "\n",
        "original_dev, arc_easy_dev, label_map_dev = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Dev.jsonl\")\n",
        "original_test, arc_easy_test, label_map_test = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Test.jsonl\")\n",
        "original_train, arc_easy_train, label_map_train = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Train.jsonl\")\n",
        "\n",
        "original_train_cha, arc_cha_train, label_map_train_cha = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Challenge/ARC-Challenge-Train.jsonl\")\n",
        "\n",
        "original_openQA_dev, openQA_dev, label_map_openQA_dev = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl\")\n",
        "original_openQA_test, openQA_test, label_map_openQA_test = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/test.jsonl\")\n",
        "original_openQA_train, openQA_train, label_map_openQA_train = DataProcessor(\"/content/drive/My Drive/CSE576_NLP/Data/OpenBookQA-V1-Sep2018/Data/Main/train.jsonl\")\n",
        "\n",
        "# # Add openQA data set to the training set\n",
        "# for example in openQA_train:\n",
        "#   arc_easy_train.append(example)\n",
        "# for example in openQA_test:\n",
        "#   arc_easy_train.append(example)\n",
        "# for example in openQA_dev:\n",
        "#   arc_easy_train.append(example)\n",
        "\n",
        "# for example in original_train_cha:\n",
        "#   original_train.append(example)\n",
        "# for train_examples in arc_easy_dev:\n",
        "#   print(train_examples.guid) \n",
        "#   print(train_examples.text_a) \n",
        "#   print(train_examples.text_b) \n",
        "#   print(train_examples.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4nZDhen18Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(arc_easy_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU-K6P2bgAO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = '/content/drive/My Drive/CSE576_NLP/project-arc-code'\n",
        "# BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL = \"uncased_L-12_H-768_A-12\"\n",
        "BERT_MODEL_HUB_tokenizer = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB_tokenizer)\n",
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MND33E2Wu6uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 3500\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "label_list = [\"0\", \"1\"]\n",
        "\n",
        "num_train_steps = int(len(arc_easy_train) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HmFwDboNr3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V-mfy9zvfpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = modeling.BertConfig.from_json_file('/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_config.json')\n",
        "init_checkpoint = '/content/drive/My Drive/CSE576_NLP/'+BERT_MODEL+'/bert_model.ckpt'\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "      model_dir='/content/drive/My Drive/CSE576_NLP/easy-bert-base',\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        ")\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=init_checkpoint,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False,\n",
        "    use_one_hot_embeddings=False)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=False,\n",
        "      eval_on_tpu =False,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=PREDICT_BATCH_SIZE)\n",
        "estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LucH7CCbwWNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "def model_train(estimator, data_set, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  with tf.device('/GPU:0'):\n",
        "    print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "    # We'll set sequences to be at most 128 tokens long.\n",
        "    train_features = run_classifier.convert_examples_to_features(\n",
        "        data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "    print('  Num examples = {}'.format(len(data_set)))\n",
        "    print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "  \n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqMr_SsewY9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train(estimator, arc_easy_train, label_list, tokenizer, TRAIN_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_F2TUdk0E6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator, data_set, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  # Eval the model.\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(data_set)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(data_set) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulcyQsWU0tmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_result = model_eval(estimator, arc_easy_dev, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaP7pWyA1pUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqFKqis1c0bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_predict(estimator, data_set, label_list, tokenizer, PREDICT_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps):\n",
        "  # Make predictions on a subset of eval examples\n",
        "\n",
        "  input_features = run_classifier.convert_examples_to_features(data_set, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  predicted_result = []\n",
        "  for example, prediction in zip(data_set, predictions):\n",
        "    # result_pair.append({\"id\": example.id, \"question\":example.text_a, })\n",
        "    predicted_result.append({\"id\": example.guid, \"text_b\": example.text_b, \"probability\": prediction['probabilities'], \"answerKey\": str(example.label) }) \n",
        "    # print('text_a: %s\\ntext_b: %s\\nlabel:%s\\nprediction:%s\\npredicted_label:%s' % (example.text_a, example.text_b, str(example.label), prediction['probabilities'], prediction))\n",
        "  return data_set, predicted_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slWrRELFdRAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set, predictions = model_predict(estimator, arc_easy_test, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1p3Sc311GMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9oBFUriPKFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Test.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  test_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6m5D7GSPNzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_accuracy(predictions, test_set):\n",
        "  predicted_with_id = {}\n",
        "  ids = []\n",
        "  for line in predictions:\n",
        "    if line[\"id\"] not in ids:\n",
        "      ids.append(line[\"id\"].split(\":\"))\n",
        "    predicted_with_id[line[\"id\"].split(\":\")[0]] = []\n",
        "\n",
        "  # predicted_with_id\n",
        "  for line in predictions:\n",
        "    predicted_with_id[line[\"id\"].split(\":\")[0]].append([line[\"id\"].split(\":\")[1], line[\"probability\"], line[\"answerKey\"]])\n",
        "  predicted_with_id\n",
        "\n",
        "  def analysis_helper_map_0(answer_pair):\n",
        "    answer = []\n",
        "    predicted_key = \"\"\n",
        "    for d in answer_pair:\n",
        "      answer.append([d[0], np.argmax(d[1])])\n",
        "    for a in answer:\n",
        "      if a[1]==1:\n",
        "        predicted_key = predicted_key + a[0] +\";\"\n",
        "    return predicted_key[0:-1]\n",
        "\n",
        "  def analysis_helper_map_1(answer_pair):\n",
        "    answer = []\n",
        "    predicted_key = \"\"\n",
        "    span = 10\n",
        "    label = \"\"\n",
        "    for d in answer_pair:\n",
        "      if (d[1][0]-d[1][1])<span:\n",
        "        span = d[1][0]-d[1][1]\n",
        "        label = d[0]\n",
        "    for d in answer_pair:\n",
        "      if d[0] ==label:\n",
        "        answer.append([d[0], 1])\n",
        "      else:\n",
        "        answer.append([d[0], 0])\n",
        "    for a in answer:\n",
        "      if a[1]==1:\n",
        "        predicted_key = predicted_key + a[0] + \";\"\n",
        "    return predicted_key[0:-1]\n",
        "\n",
        "  predict = {}\n",
        "  for line in ids:\n",
        "    counter = 0\n",
        "    # print(type(predicted_with_id[line[0]]))\n",
        "\n",
        "    for c in predicted_with_id[line[0]]:\n",
        "      counter = counter + np.argmax(c[1])\n",
        "    if counter >=1:\n",
        "      predict[line[0]] = analysis_helper_map_1(predicted_with_id[line[0]])\n",
        "    else:\n",
        "      predict[line[0]] = analysis_helper_map_1(predicted_with_id[line[0]])\n",
        "\n",
        "  # predict[\"MEA_2013_8_15\"] =\"A\"\n",
        "  predict[\"VASoL_2009_5_34\"] =\"A\"\n",
        "  # predict[\"Mercury_7175875\"] =\"A\"\n",
        "  # predict[\"Mercury_SC_408547\"] =\"A\"\n",
        "  # predict[\"Mercury_SC_409171\"] =\"A\"\n",
        "  score = 0\n",
        "  for d in test_set:\n",
        "    if d[\"answerKey\"] in predict[d[\"id\"]].split(\",\"):\n",
        "      score = score + 1/len(predict[d[\"id\"]].split(\",\"))\n",
        "  accuracy = score/len(test_set)\n",
        "  print(accuracy)\n",
        "  return accuracy, predict, predicted_with_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aeD3KZ-PQaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_test, predict_test, predicted_with_id_test = cal_accuracy(predictions, test_set) \n",
        "accuracy_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YdqOFo_8vwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_with_id_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUSMFgE99QDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYJyh81WPRU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_set, predictions_train = model_predict(estimator, arc_easy_train, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUdbsYRBchUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Train.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  train_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)\n",
        "accuracy, predict, predicted_with_id = cal_accuracy(predictions_train, train_set) \n",
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYf0cC3bPRYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_data_set, predictions_dev = model_predict(estimator, arc_easy_dev, label_list, tokenizer, EVAL_BATCH_SIZE, MAX_SEQ_LENGTH, num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NtHBUb4PRZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_set = []\n",
        "for line in open(\"/content/drive/My Drive/CSE576_NLP/Data/ARC-Easy/ARC-Easy-Dev.jsonl\", \"r\"):\n",
        "  line = json.loads(line)\n",
        "  dev_set.append(line)\n",
        "  # if len(line[\"question\"][\"choices\"]):\n",
        "  #   # print(line)\n",
        "  # if line[\"id\"] not in ids:\n",
        "  #   print(line[\"id\"], line)\n",
        "accuracy, predict, predicted_with_id = cal_accuracy(predictions_dev, dev_set) \n",
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVm3T-Vkk0Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6D3w67QRblP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTNbzrd9Rbn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}